Notes:
lower batch size is best predictor for low loss,
which maybe means that these low losses are achieved by lucky draw of data,
so we might need a validation set to confirm the results



Top 100 results (sorted by loss):
Rank 1, loss = 1.381273627281189
    act_fn: Tanh()
    width: 256
    depth: 8
    lr: 0.001
    loss_fn_batch_sizes: [100, 50, 20, 20]
Rank 2, loss = 1.4700781106948853
    act_fn: Tanh()
    width: 64
    depth: 4
    lr: 0.001
    loss_fn_batch_sizes: [100, 50, 20, 20]
Rank 3, loss = 1.5192853212356567
    act_fn: Tanh()
    width: 256
    depth: 4
    lr: 1e-05
    loss_fn_batch_sizes: [100, 50, 20, 20]
Rank 4, loss = 1.5657929182052612
    act_fn: PolyReLU(n=2)
    width: 64
    depth: 8
    lr: 0.0001
    loss_fn_batch_sizes: [100, 50, 20, 20]
Rank 5, loss = 1.6179426908493042
    act_fn: Tanh()
    width: 128
    depth: 2
    lr: 0.001
    loss_fn_batch_sizes: [100, 50, 20, 20]
Rank 6, loss = 1.6251033544540405
    act_fn: Tanh()
    width: 256
    depth: 4
    lr: 0.0001
    loss_fn_batch_sizes: [100, 50, 20, 20]
Rank 7, loss = 1.6273268461227417
    act_fn: Tanh()
    width: 256
    depth: 8
    lr: 1e-05
    loss_fn_batch_sizes: [100, 50, 20, 20]
Rank 8, loss = 1.6492644548416138
    act_fn: SiLU()
    width: 128
    depth: 8
    lr: 0.0001
    loss_fn_batch_sizes: [100, 50, 20, 20]
Rank 9, loss = 2.177135944366455
    act_fn: Tanh()
    width: 64
    depth: 8
    lr: 0.001
    loss_fn_batch_sizes: [500, 80, 40, 40]
Rank 10, loss = 2.3053271770477295
    act_fn: PolyReLU(n=2)
    width: 128
    depth: 2
    lr: 0.001
    loss_fn_batch_sizes: [500, 80, 40, 40]
Rank 11, loss = 2.306534767150879
    act_fn: ReLU()
    width: 256
    depth: 2
    lr: 0.001
    loss_fn_batch_sizes: [100, 50, 20, 20]
Rank 12, loss = 2.3094561100006104
    act_fn: PolyReLU(n=2)
    width: 64
    depth: 8
    lr: 0.0001
    loss_fn_batch_sizes: [500, 80, 40, 40]
Rank 13, loss = 2.310682535171509
    act_fn: Tanh()
    width: 128
    depth: 2
    lr: 0.001
    loss_fn_batch_sizes: [800, 100, 50, 50]
Rank 14, loss = 2.325113296508789
    act_fn: Tanh()
    width: 256
    depth: 2
    lr: 0.001
    loss_fn_batch_sizes: [500, 80, 40, 40]
Rank 15, loss = 2.361717700958252
    act_fn: SiLU()
    width: 64
    depth: 2
    lr: 0.0001
    loss_fn_batch_sizes: [800, 100, 50, 50]
Rank 16, loss = 2.3684141635894775
    act_fn: Tanh()
    width: 256
    depth: 4
    lr: 0.0001
    loss_fn_batch_sizes: [500, 80, 40, 40]
Rank 17, loss = 2.3687584400177
    act_fn: Tanh()
    width: 256
    depth: 8
    lr: 0.001
    loss_fn_batch_sizes: [500, 80, 40, 40]
Rank 18, loss = 2.3819077014923096
    act_fn: PolyReLU(n=3)
    width: 64
    depth: 2
    lr: 0.001
    loss_fn_batch_sizes: [500, 80, 40, 40]
Rank 19, loss = 2.387186288833618
    act_fn: PolyReLU(n=2)
    width: 128
    depth: 4
    lr: 0.0001
    loss_fn_batch_sizes: [500, 80, 40, 40]
Rank 20, loss = 2.392423391342163
    act_fn: SiLU()
    width: 256
    depth: 4
    lr: 0.0001
    loss_fn_batch_sizes: [800, 100, 50, 50]
Rank 21, loss = 2.4175214767456055
    act_fn: SiLU()
    width: 128
    depth: 2
    lr: 0.0001
    loss_fn_batch_sizes: [800, 100, 50, 50]
Rank 22, loss = 2.426687240600586
    act_fn: PolyReLU(n=3)
    width: 64
    depth: 4
    lr: 0.001
    loss_fn_batch_sizes: [100, 50, 20, 20]
Rank 23, loss = 2.4384961128234863
    act_fn: SiLU()
    width: 128
    depth: 4
    lr: 0.0001
    loss_fn_batch_sizes: [1000, 300, 100, 100]
Rank 24, loss = 2.4508659839630127
    act_fn: SiLU()
    width: 128
    depth: 8
    lr: 1e-05
    loss_fn_batch_sizes: [1000, 300, 100, 100]
Rank 25, loss = 2.46673321723938
    act_fn: SiLU()
    width: 256
    depth: 8
    lr: 1e-05
    loss_fn_batch_sizes: [1000, 300, 100, 100]
Rank 26, loss = 2.47200083732605
    act_fn: SiLU()
    width: 128
    depth: 8
    lr: 0.001
    loss_fn_batch_sizes: [800, 100, 50, 50]
Rank 27, loss = 2.517436981201172
    act_fn: ReLU()
    width: 128
    depth: 8
    lr: 0.001
    loss_fn_batch_sizes: [100, 50, 20, 20]
Rank 28, loss = 2.5264084339141846
    act_fn: SiLU()
    width: 256
    depth: 2
    lr: 0.0001
    loss_fn_batch_sizes: [1000, 300, 100, 100]
Rank 29, loss = 2.528856039047241
    act_fn: Tanh()
    width: 128
    depth: 4
    lr: 1e-05
    loss_fn_batch_sizes: [1000, 300, 100, 100]
Rank 30, loss = 2.534881830215454
    act_fn: PolyReLU(n=2)
    width: 256
    depth: 4
    lr: 0.0001
    loss_fn_batch_sizes: [1000, 300, 100, 100]
Rank 31, loss = 2.535957098007202
    act_fn: ReLU()
    width: 64
    depth: 8
    lr: 0.001
    loss_fn_batch_sizes: [100, 50, 20, 20]
Rank 32, loss = 2.5517919063568115
    act_fn: Tanh()
    width: 64
    depth: 8
    lr: 0.0001
    loss_fn_batch_sizes: [800, 100, 50, 50]
Rank 33, loss = 2.5521013736724854
    act_fn: PolyReLU(n=2)
    width: 128
    depth: 2
    lr: 0.001
    loss_fn_batch_sizes: [1000, 300, 100, 100]
Rank 34, loss = 2.558418035507202
    act_fn: SiLU()
    width: 256
    depth: 8
    lr: 0.0001
    loss_fn_batch_sizes: [1000, 300, 100, 100]
Rank 35, loss = 2.5607426166534424
    act_fn: PolyReLU(n=3)
    width: 64
    depth: 2
    lr: 0.0001
    loss_fn_batch_sizes: [800, 100, 50, 50]
Rank 36, loss = 2.5616753101348877
    act_fn: Tanh()
    width: 128
    depth: 8
    lr: 1e-05
    loss_fn_batch_sizes: [800, 100, 50, 50]
Rank 37, loss = 2.5673322677612305
    act_fn: SiLU()
    width: 128
    depth: 8
    lr: 0.001
    loss_fn_batch_sizes: [1000, 300, 100, 100]
Rank 38, loss = 2.568006992340088
    act_fn: PolyReLU(n=2)
    width: 64
    depth: 2
    lr: 0.0001
    loss_fn_batch_sizes: [800, 100, 50, 50]
Rank 39, loss = 2.5738186836242676
    act_fn: PolyReLU(n=2)
    width: 64
    depth: 4
    lr: 0.0001
    loss_fn_batch_sizes: [800, 100, 50, 50]
Rank 40, loss = 2.5930707454681396
    act_fn: Tanh()
    width: 256
    depth: 8
    lr: 0.001
    loss_fn_batch_sizes: [1000, 300, 100, 100]
Rank 41, loss = 2.601524591445923
    act_fn: Tanh()
    width: 256
    depth: 2
    lr: 0.001
    loss_fn_batch_sizes: [1000, 300, 100, 100]
Rank 42, loss = 2.60170841217041
    act_fn: PolyReLU(n=2)
    width: 64
    depth: 4
    lr: 0.001
    loss_fn_batch_sizes: [1000, 300, 100, 100]
Rank 43, loss = 2.6126091480255127
    act_fn: Tanh()
    width: 128
    depth: 8
    lr: 0.0001
    loss_fn_batch_sizes: [1000, 300, 100, 100]
Rank 44, loss = 2.6290125846862793
    act_fn: PolyReLU(n=2)
    width: 64
    depth: 2
    lr: 0.001
    loss_fn_batch_sizes: [1000, 300, 100, 100]
Rank 45, loss = 2.635425329208374
    act_fn: ReLU()
    width: 64
    depth: 4
    lr: 0.0001
    loss_fn_batch_sizes: [100, 50, 20, 20]
Rank 46, loss = 2.6359310150146484
    act_fn: PolyReLU(n=3)
    width: 128
    depth: 4
    lr: 0.001
    loss_fn_batch_sizes: [100, 50, 20, 20]
Rank 47, loss = 2.6626391410827637
    act_fn: ReLU()
    width: 128
    depth: 2
    lr: 1e-05
    loss_fn_batch_sizes: [100, 50, 20, 20]
Rank 48, loss = 2.707089424133301
    act_fn: Tanh()
    width: 64
    depth: 2
    lr: 0.001
    loss_fn_batch_sizes: [3000, 1000, 200, 200]
Rank 49, loss = 2.7254984378814697
    act_fn: PolyReLU(n=2)
    width: 128
    depth: 2
    lr: 0.001
    loss_fn_batch_sizes: [3000, 1000, 200, 200]
Rank 50, loss = 2.734510898590088
    act_fn: SiLU()
    width: 256
    depth: 2
    lr: 0.0001
    loss_fn_batch_sizes: [3000, 1000, 200, 200]
Rank 51, loss = 2.7378671169281006
    act_fn: SiLU()
    width: 64
    depth: 8
    lr: 0.0001
    loss_fn_batch_sizes: [3000, 1000, 200, 200]
Rank 52, loss = 2.763906955718994
    act_fn: PolyReLU(n=3)
    width: 64
    depth: 4
    lr: 1e-05
    loss_fn_batch_sizes: [100, 50, 20, 20]
Rank 53, loss = 2.7737629413604736
    act_fn: PolyReLU(n=2)
    width: 64
    depth: 8
    lr: 1e-05
    loss_fn_batch_sizes: [100, 50, 20, 20]
Rank 54, loss = 2.8368539810180664
    act_fn: PolyReLU(n=3)
    width: 64
    depth: 2
    lr: 0.0001
    loss_fn_batch_sizes: [3000, 1000, 200, 200]
Rank 55, loss = 2.8533315658569336
    act_fn: SiLU()
    width: 256
    depth: 2
    lr: 1e-05
    loss_fn_batch_sizes: [100, 50, 20, 20]
Rank 56, loss = 2.8984200954437256
    act_fn: PolyReLU(n=3)
    width: 128
    depth: 8
    lr: 0.0001
    loss_fn_batch_sizes: [100, 50, 20, 20]
Rank 57, loss = 2.9183802604675293
    act_fn: SiLU()
    width: 64
    depth: 8
    lr: 1e-05
    loss_fn_batch_sizes: [100, 50, 20, 20]
Rank 58, loss = 2.9191932678222656
    act_fn: Tanh()
    width: 64
    depth: 8
    lr: 1e-05
    loss_fn_batch_sizes: [3000, 1000, 200, 200]
Rank 59, loss = 2.9373815059661865
    act_fn: PolyReLU(n=2)
    width: 256
    depth: 4
    lr: 1e-05
    loss_fn_batch_sizes: [500, 80, 40, 40]
Rank 60, loss = 3.213716745376587
    act_fn: PolyReLU(n=2)
    width: 256
    depth: 4
    lr: 1e-05
    loss_fn_batch_sizes: [800, 100, 50, 50]
Rank 61, loss = 3.937051773071289
    act_fn: ReLU()
    width: 256
    depth: 4
    lr: 0.0001
    loss_fn_batch_sizes: [500, 80, 40, 40]
Rank 62, loss = 4.011909484863281
    act_fn: PolyReLU(n=2)
    width: 64
    depth: 2
    lr: 1e-05
    loss_fn_batch_sizes: [500, 80, 40, 40]
Rank 63, loss = 4.072672367095947
    act_fn: ReLU()
    width: 256
    depth: 8
    lr: 0.0001
    loss_fn_batch_sizes: [500, 80, 40, 40]
Rank 64, loss = 4.091373920440674
    act_fn: Tanh()
    width: 64
    depth: 4
    lr: 1e-05
    loss_fn_batch_sizes: [500, 80, 40, 40]
Rank 65, loss = 4.18467378616333
    act_fn: ReLU()
    width: 256
    depth: 8
    lr: 1e-05
    loss_fn_batch_sizes: [500, 80, 40, 40]
Rank 66, loss = 4.2195725440979
    act_fn: Tanh()
    width: 64
    depth: 2
    lr: 0.0001
    loss_fn_batch_sizes: [800, 100, 50, 50]
Rank 67, loss = 4.23769998550415
    act_fn: ReLU()
    width: 256
    depth: 4
    lr: 0.0001
    loss_fn_batch_sizes: [800, 100, 50, 50]
Rank 68, loss = 4.3034563064575195
    act_fn: ReLU()
    width: 256
    depth: 4
    lr: 1e-05
    loss_fn_batch_sizes: [800, 100, 50, 50]
Rank 69, loss = 4.3036789894104
    act_fn: ReLU()
    width: 256
    depth: 2
    lr: 1e-05
    loss_fn_batch_sizes: [1000, 300, 100, 100]
Rank 70, loss = 4.367358684539795
    act_fn: ReLU()
    width: 128
    depth: 4
    lr: 1e-05
    loss_fn_batch_sizes: [1000, 300, 100, 100]
Rank 71, loss = 4.370172023773193
    act_fn: ReLU()
    width: 64
    depth: 2
    lr: 0.0001
    loss_fn_batch_sizes: [1000, 300, 100, 100]
Rank 72, loss = 4.37253999710083
    act_fn: PolyReLU(n=2)
    width: 64
    depth: 8
    lr: 0.001
    loss_fn_batch_sizes: [800, 100, 50, 50]
Rank 73, loss = 4.427326679229736
    act_fn: PolyReLU(n=2)
    width: 256
    depth: 8
    lr: 0.001
    loss_fn_batch_sizes: [800, 100, 50, 50]
Rank 74, loss = 4.450399875640869
    act_fn: ReLU()
    width: 64
    depth: 4
    lr: 0.001
    loss_fn_batch_sizes: [800, 100, 50, 50]
Rank 75, loss = 4.504302024841309
    act_fn: ReLU()
    width: 128
    depth: 8
    lr: 0.0001
    loss_fn_batch_sizes: [1000, 300, 100, 100]
Rank 76, loss = 4.512409687042236
    act_fn: ReLU()
    width: 256
    depth: 2
    lr: 0.0001
    loss_fn_batch_sizes: [800, 100, 50, 50]
Rank 77, loss = 4.515172004699707
    act_fn: ReLU()
    width: 256
    depth: 8
    lr: 0.001
    loss_fn_batch_sizes: [1000, 300, 100, 100]
Rank 78, loss = 4.5162434577941895
    act_fn: PolyReLU(n=2)
    width: 64
    depth: 2
    lr: 1e-05
    loss_fn_batch_sizes: [800, 100, 50, 50]
Rank 79, loss = 4.517866134643555
    act_fn: ReLU()
    width: 64
    depth: 4
    lr: 0.0001
    loss_fn_batch_sizes: [800, 100, 50, 50]
Rank 80, loss = 4.558172702789307
    act_fn: PolyReLU(n=2)
    width: 128
    depth: 8
    lr: 1e-05
    loss_fn_batch_sizes: [800, 100, 50, 50]
Rank 81, loss = 4.575734615325928
    act_fn: ReLU()
    width: 64
    depth: 4
    lr: 0.0001
    loss_fn_batch_sizes: [1000, 300, 100, 100]
Rank 82, loss = 4.578408241271973
    act_fn: SiLU()
    width: 64
    depth: 4
    lr: 1e-05
    loss_fn_batch_sizes: [800, 100, 50, 50]
Rank 83, loss = 4.58439302444458
    act_fn: Tanh()
    width: 256
    depth: 4
    lr: 1e-05
    loss_fn_batch_sizes: [800, 100, 50, 50]
Rank 84, loss = 4.592132568359375
    act_fn: ReLU()
    width: 64
    depth: 8
    lr: 1e-05
    loss_fn_batch_sizes: [800, 100, 50, 50]
Rank 85, loss = 4.740268707275391
    act_fn: PolyReLU(n=3)
    width: 128
    depth: 8
    lr: 1e-05
    loss_fn_batch_sizes: [1000, 300, 100, 100]
Rank 86, loss = 4.767544269561768
    act_fn: PolyReLU(n=3)
    width: 128
    depth: 2
    lr: 1e-05
    loss_fn_batch_sizes: [1000, 300, 100, 100]
Rank 87, loss = 4.831548690795898
    act_fn: ReLU()
    width: 256
    depth: 8
    lr: 0.0001
    loss_fn_batch_sizes: [3000, 1000, 200, 200]
Rank 88, loss = 4.891446113586426
    act_fn: ReLU()
    width: 128
    depth: 8
    lr: 1e-05
    loss_fn_batch_sizes: [3000, 1000, 200, 200]
Rank 89, loss = 4.891561031341553
    act_fn: ReLU()
    width: 128
    depth: 8
    lr: 0.0001
    loss_fn_batch_sizes: [3000, 1000, 200, 200]
Rank 90, loss = 4.898447513580322
    act_fn: ReLU()
    width: 64
    depth: 4
    lr: 1e-05
    loss_fn_batch_sizes: [3000, 1000, 200, 200]
Rank 91, loss = 4.918276309967041
    act_fn: Tanh()
    width: 64
    depth: 2
    lr: 1e-05
    loss_fn_batch_sizes: [3000, 1000, 200, 200]
Rank 92, loss = 4.949972629547119
    act_fn: ReLU()
    width: 256
    depth: 8
    lr: 0.001
    loss_fn_batch_sizes: [3000, 1000, 200, 200]
Rank 93, loss = 4.963106632232666
    act_fn: PolyReLU(n=3)
    width: 256
    depth: 2
    lr: 1e-05
    loss_fn_batch_sizes: [3000, 1000, 200, 200]
Rank 94, loss = 4.9690117835998535
    act_fn: PolyReLU(n=3)
    width: 128
    depth: 8
    lr: 0.0001
    loss_fn_batch_sizes: [3000, 1000, 200, 200]
Rank 95, loss = 4.9894280433654785
    act_fn: PolyReLU(n=3)
    width: 256
    depth: 8
    lr: 0.0001
    loss_fn_batch_sizes: [3000, 1000, 200, 200]
Rank 96, loss = 4.991158485412598
    act_fn: ReLU()
    width: 64
    depth: 2
    lr: 0.001
    loss_fn_batch_sizes: [3000, 1000, 200, 200]
Rank 97, loss = 5.0256266593933105
    act_fn: ReLU()
    width: 256
    depth: 2
    lr: 0.0001
    loss_fn_batch_sizes: [3000, 1000, 200, 200]
Rank 98, loss = 5.030173301696777
    act_fn: PolyReLU(n=3)
    width: 128
    depth: 4
    lr: 0.001
    loss_fn_batch_sizes: [3000, 1000, 200, 200]
Rank 99, loss = 5.074953556060791
    act_fn: PolyReLU(n=3)
    width: 256
    depth: 4
    lr: 0.0001
    loss_fn_batch_sizes: [3000, 1000, 200, 200]
Rank 100, loss = 5.095386981964111
    act_fn: PolyReLU(n=3)
    width: 256
    depth: 8
    lr: 1e-05
    loss_fn_batch_sizes: [3000, 1000, 200, 200]

--------------------------------------------------
